{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "#Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "#Models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.adapt import MLkNN\n",
    "\n",
    "#Scoring Metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\Admin\\PycharmProjects\\Thesis2.0\\django_thesis\\Dataset\\trainingData.csv'\n",
    "trainingData = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Check the structure of the data after it's loaded \n",
    "#(print the number of rows and columns).\n",
    "num_rows, num_cols  = trainingData.shape\n",
    "print('Number of columns: {}'.format(num_cols))\n",
    "print('Number of rows: {}'.format(num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#check the statistics of the data per columns\n",
    "trainingData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "trainingData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Check the columns names\n",
    "col_names = trainingData.columns.values\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "missing_values_count = trainingData.isnull().sum()\n",
    "#uncomment this if you want to see the count of missing data per column\n",
    "#missing_values_count\n",
    "\n",
    "# how many total missing values do we have?\n",
    "total_cells = np.product(trainingData.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "missing_percent = (total_missing/total_cells) * 100\n",
    "\n",
    "print('Percent of missing data = {}%'.format(missing_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Assess unique values per building columns\n",
    "unique_floors = trainingData[\"FLOOR\"].unique()\n",
    "unique_bldgs = trainingData[\"BUILDINGID\"].unique()\n",
    "unique_spaceid = trainingData[\"SPACEID\"].unique()\n",
    "unique_rpos = trainingData[\"RELATIVEPOSITION\"].unique()\n",
    "unique_users = trainingData[\"USERID\"].unique()\n",
    "print('Unique Floors : {}'.format(unique_floors))\n",
    "print('Unique Buildings : {}'.format(unique_bldgs))\n",
    "print('Unique Space IDs : {}'.format(unique_spaceid))\n",
    "print('Unique Relative Positions : {}'.format(unique_rpos))\n",
    "print('Unique Users : {}'.format(unique_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Map the data i.e. see the campus\n",
    "trainingData.plot(kind=\"scatter\", x=\"LONGITUDE\", y=\"LATITUDE\", alpha=0.2)\n",
    "plt.savefig('data_map.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#map the data by user ID\n",
    "#i.e. see how much of the information in each building was collected by how many users\n",
    "trainingData.plot(kind=\"scatter\", x=\"LONGITUDE\", y=\"LATITUDE\", alpha=0.4, figsize=(10,7),\n",
    "    c=\"USERID\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "    sharex=False)\n",
    "plt.savefig('user_map2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# plot the correlations between the WAP features\n",
    "corr_matrix = trainingData.corr()\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "sns.heatmap(corr_matrix,xticklabels=False, yticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# plot the histograms of the attributes\n",
    "trainingData.iloc[:, 520:529].hist(bins=50, figsize=(20,15))\n",
    "plt.savefig(\"attribute_histogram_plots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "attributes = [\"BUILDINGID\",\"FLOOR\", \"LATITUDE\", \"LONGITUDE\", \"SPACEID\", \"RELATIVEPOSITION\"]\n",
    "scatter_matrix(trainingData[attributes], figsize=(12, 8))\n",
    "plt.savefig('matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Perform feature trimming, and engineering for trainingData\n",
    "    Will also be applied to validationData\n",
    "    \n",
    "    INPUT: trainingData DataFrame\n",
    "    OUTPUT: Trimmed and cleaned trainingData DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reverse the representation for the values. 100=0 and teh values range from 0-105 (weakest to strongest)\n",
    "    #\"The intensity values are represented as negative integer values ranging -104dBm (extremely poor signal) to 0dbM.\n",
    "    #The positive value 100 is used to denote when a WAP was not detected.\"\n",
    "    df.iloc[:, 0:520] = np.where(df.iloc[:, 0:520] <= 0, \n",
    "                df.iloc[:, 0:520] + 105, \n",
    "                df.iloc[:, 0:520] - 100)\n",
    "    \n",
    "    # remove selected columns... \n",
    "    columns_removed = ['USERID', 'PHONEID','TIMESTAMP']\n",
    "    for col in columns_removed:\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "   \n",
    "    # Return the cleaned dataframe.\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Apply Cleaning\n",
    "\n",
    "trainingData  = clean_data(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Separates trainingData into Features and Targets\n",
    "    Will also be applied to validationData\n",
    "    \n",
    "    INPUT: Cleaned trainingData DataFrame\n",
    "    OUTPUT: trainingData as Features and Targets\n",
    "    \"\"\"\n",
    "    \n",
    "    global X\n",
    "    global y\n",
    "    # split the data set into features and targets(Floor and BuildingID)\n",
    "    X = df.drop(['LONGITUDE', 'LATITUDE','FLOOR','BUILDINGID', 'SPACEID','RELATIVEPOSITION'], axis=1)\n",
    "    y = df[['BUILDINGID', 'FLOOR']]\n",
    "    \n",
    "    \n",
    "    #create Dummies for the targets to feed into the model\n",
    "    y = pd.get_dummies(data=y, columns=['BUILDINGID', 'FLOOR'])\n",
    "    \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Apply preprocessing\n",
    "\n",
    "X, y = preprocess_data(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def split_data(preprocess_data):\n",
    "# TO AVOID OVERFITTING: Split the training data into training and testing sets \n",
    "    global X_train\n",
    "    global X_test\n",
    "    global y_train\n",
    "    global y_test\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state = 42,\n",
    "                                                        shuffle=True)\n",
    "\n",
    "    # Show the results of the split\n",
    "    print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "    print(\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Apply split data\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Scale Data with Standard Scaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fit only the training set\n",
    "#this will help us transform the validation data \n",
    "scaler.fit(X_train)\n",
    "    \n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Apply PCA while keeping 95% of the variation in the data\n",
    "pca = PCA(.95)\n",
    "\n",
    "    \n",
    "#Fit only the training set    \n",
    "pca.fit(X_train)\n",
    "\n",
    "# Apply PCA transform to both the training set and the test set.    \n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(\"Number of PCA Components = {}.\".format(pca.n_components_))\n",
    "#print(pca.n_components_)\n",
    "print(\"Total Variance Explained by PCA Components = {}.\".format(pca.explained_variance_ratio_.sum()))\n",
    "#print(pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def pca_plot(pca):\n",
    "    '''\n",
    "    Creates a scree plot associated with the principal components \n",
    "    \n",
    "    INPUT: pca - the result of instantian of PCA in scikit learn\n",
    "            \n",
    "    OUTPUT:\n",
    "            None\n",
    "    '''\n",
    "    num_components = len(pca.explained_variance_ratio_)\n",
    "    ind = np.arange(num_components)\n",
    "    vals = pca.explained_variance_ratio_\n",
    " \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = plt.subplot(111)\n",
    "    cumvals = np.cumsum(vals)\n",
    "    ax.bar(ind, vals)\n",
    "    ax.plot(ind, cumvals)\n",
    "    for i in range(num_components):\n",
    "        ax.annotate(r\"%s%%\" % ((str(vals[i]*100)[:4])), (ind[i]+0.2, vals[i]), va=\"bottom\", ha=\"center\", fontsize=12)\n",
    " \n",
    "    ax.xaxis.set_tick_params(width=0)\n",
    "    ax.yaxis.set_tick_params(width=2, length=12)\n",
    " \n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_ylabel(\"Variance Explained (%)\")\n",
    "    plt.title('Explained Variance Per Principal Component')\n",
    "    \n",
    "pca_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Create sparse matrices to run the scikit multilearn algorithms\n",
    "\n",
    "X_train_pca = lil_matrix(X_train_pca).toarray()\n",
    "y_train = lil_matrix(y_train).toarray()\n",
    "X_test_pca = lil_matrix(X_test_pca).toarray()\n",
    "y_test = lil_matrix(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
